distributed:
    backend: gloo
    world_size: 8
    master_addr: localhost
    master_port: '12345'
algo:
    cls_name: PPO
    seg_len: 256
    opt_epochs: 4
    learner_batch_size: 64
    clip_param_init: 0.2
    clip_param_final: 0.0
    ent_coef_init: 0.01
    ent_coef_final: 0.01
    vf_loss_coef: 0.5
    vf_loss_cls: MSELoss
    vf_loss_clipping: True
    gae_lambda:
        extrinsic: 0.95
    gae_extra_steps: 256
    discount_gamma:
        extrinsic: 0.99
    standardize_adv: True
    use_pcgrad: False
    stats_memory_len: 100
    checkpoint_frequency: 100
    max_steps: 10000000
env:
    id: 'PongNoFrameskip-v4'
    wrappers:
        AtariWrapper: {}
        DeepmindWrapper: {frame_stack: True, lazy: False}
networks:
    policy_net:
        preprocessing:
            ToChannelMajor: {}
        architecture:
            cls_name: NatureCNN
            cls_args: {img_channels: 4, ortho_init: False}
        predictors:
            policy:
                cls_name: LinearCategoricalPolicyHead
                cls_args: {num_features: 512, ortho_init: False}
            value_extrinsic:
                cls_name: LinearValueHead
                cls_args: {num_features: 512, ortho_init: False}
        optimizer:
            cls_name: Adam
            cls_args: {lr: 0.001, betas: [0.9, 0.999], eps: 0.00000001}
        scheduler:
            cls_name: OneCycleLR
            cls_args:
                max_lr: 0.001
                total_steps: 85938
                pct_start: 0.0
                anneal_strategy: linear
                cycle_momentum: False
                div_factor: 1.0
    value_net:
        use_shared_architecture: True